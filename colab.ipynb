{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7aJhsgLAWvO"
      },
      "source": [
        "# Style-Bert-VITS2 (ver 2.7.0) のGoogle Colabでの学習\n",
        "\n",
        "Google Colab上でStyle-Bert-VITS2の学習を行うことができます。\n",
        "\n",
        "このnotebookでは、通常使用ではあなたのGoogle Driveにフォルダ`Style-Bert-VITS2`を作り、その内部での作業を行います。他のフォルダには触れません。\n",
        "Google Driveを使わない場合は、初期設定のところで適切なパスを指定してください。\n",
        "\n",
        "## 流れ\n",
        "\n",
        "### 学習を最初からやりたいとき\n",
        "上から順に実行していけばいいです。音声合成に必要なファイルはGoogle Driveの`Style-Bert-VITS2/model_assets/`に保存されます。また、途中経過も`Style-Bert-VITS2/Data/`に保存されるので、学習を中断したり、途中から再開することもできます。\n",
        "\n",
        "### 学習を途中から再開したいとき\n",
        "0と1を行い、3の前処理は飛ばして、4から始めてください。スタイル分け5は、学習が終わったら必要なら行ってください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-gAIubBAWvQ"
      },
      "source": [
        "## 0. 環境構築\n",
        "\n",
        "Style-Bert-VITS2の環境をcolab上に構築します。ランタイムがT4等のGPUバックエンドになっていることを確認し、実行してください。\n",
        "\n",
        "**注意**: このセルを実行した後に「セッションがクラッシュしました」「不明な理由により、セッションがクラッシュしました。」等の警告が出ますが、**無視してそのまま先へ**進んでください。（一度ランタイムを再起動させてnumpy<2を強制させるため `exit()` を呼んでいることからの措置です。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GNj8JyDAlm2",
        "outputId": "d00d797a-0f81-4981-ec8e-cc4f2b289b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.9.10 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "fatal: destination path 'Style-Bert-VITS2' already exists and is not an empty directory.\n",
            "/content/Style-Bert-VITS2\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m26 packages\u001b[0m \u001b[2min 230ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PATH\"] += \":/root/.cargo/bin\"\n",
        "\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "!git clone https://github.com/litagin02/Style-Bert-VITS2.git\n",
        "%cd Style-Bert-VITS2/\n",
        "!uv pip install --system -r requirements-colab.txt --no-progress\n",
        "!python initialize.py --skip_default_models\n",
        "\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5z1nzkvAWvR",
        "outputId": "7ed64f5b-f858-4ea5-9d85-dc19c4c3e984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google driveを使う方はこちらを実行してください。\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9apXzcAWvR"
      },
      "source": [
        "## 1. 初期設定\n",
        "\n",
        "学習とその結果を保存するディレクトリ名を指定します。\n",
        "Google driveの場合はそのまま実行、カスタマイズしたい方は変更して実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gO3OwZV1AWvR",
        "outputId": "29c1a52b-6acd-4a9f-b090-8e839440e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Style-Bert-VITS2\n"
          ]
        }
      ],
      "source": [
        "# 作業ディレクトリを移動\n",
        "%cd /content/Style-Bert-VITS2/\n",
        "\n",
        "# 学習に必要なファイルや途中経過が保存されるディレクトリ\n",
        "dataset_root = \"/content/drive/MyDrive/Style-Bert-VITS2/Data\"\n",
        "\n",
        "# 学習結果（音声合成に必要なファイルたち）が保存されるディレクトリ\n",
        "assets_root = \"/content/drive/MyDrive/Style-Bert-VITS2/model_assets\"\n",
        "\n",
        "import yaml\n",
        "\n",
        "\n",
        "with open(\"configs/paths.yml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump({\"dataset_root\": dataset_root, \"assets_root\": assets_root}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA_yLeezAWvS"
      },
      "source": [
        "## 2. 学習に使うデータ準備\n",
        "\n",
        "すでに音声ファイル（1ファイル2-12秒程度）とその書き起こしデータがある場合は2.2を、ない場合は2.1を実行してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s9gOnTCAWvS"
      },
      "source": [
        "### 2.1 音声ファイルからのデータセットの作成（ある人はスキップ可）\n",
        "\n",
        "音声ファイル（1ファイル2-12秒程度）とその書き起こしのデータセットを持っていない方は、（日本語の）音声ファイルのみから以下の手順でデータセットを作成することができます。Google drive上の`Style-Bert-VITS2/inputs/`フォルダに音声ファイル（wavやmp3等の通常の音声ファイル形式、1ファイルでも複数ファイルでも可）を置いて、下を実行すると、データセットが作られ、自動的に正しい場所へ配置されます。\n",
        "\n",
        "**2024-06-02のVer 2.5以降**、`inputs/`フォルダにサブフォルダを2個以上作ってそこへ音声ファイルをスタイルに応じて振り分けて置くと、学習の際にサブディレクトリに応じたスタイルが自動的に作成されます。デフォルトスタイルのみでよい場合や手動でスタイルを後で作成する場合は`inputs/`直下へ入れれば大丈夫です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fXCTPuiAWvS",
        "outputId": "5962cfe8-f8b1-47b5-9d19-4b1eee788c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.2.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.6.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.21 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (1.23.2)\n",
            "Collecting av>=11 (from faster-whisper)\n",
            "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (1.2.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Downloading faster_whisper-1.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ctranslate2, av, faster-whisper\n",
            "Successfully installed av-16.0.1 ctranslate2-4.6.1 faster-whisper-1.2.1\n",
            "11-20 14:25:36 |  INFO  | slice.py:167 | Found 2 audio files.\n",
            "11-20 14:25:36 |WARNING | slice.py:169 | Output directory /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/raw already exists, deleting...\n",
            "Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "/root/.cache/torch/hub/litagin02_silero-vad_master/utils_vad.py:127: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  audio_backends = torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "100%|##########| 2/2 [00:04<00:00,  2.04s/it]\n",
            "11-20 14:25:41 |  INFO  | slice.py:265 | Slice done! Total time: 2.83 min, 25 files.\n",
            "11-20 14:25:44 |  INFO  | transcribe.py:157 | Found 13 WAV files\n",
            "11-20 14:25:45 |  INFO  | transcribe.py:182 | Loading faster-whisper model (large-v3) with compute_type=bfloat16\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\n",
            "vocabulary.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "config.json: 2.39kB [00:00, 8.55MB/s]\n",
            "\n",
            "\n",
            "preprocessor_config.json: 100% 340/340 [00:00<00:00, 1.56MB/s]\n",
            "vocabulary.json: 1.07MB [00:00, 49.6MB/s]\n",
            "tokenizer.json: 2.48MB [00:00, 65.4MB/s]\n",
            "model.bin: 100% 3.09G/3.09G [00:24<00:00, 124MB/s]\n",
            "11-20 14:26:13 |WARNING | transcribe.py:188 | Failed to load model, so use `auto` compute_type: Requested bfloat16 compute type, but the target device or backend do not support efficient bfloat16 computation.\n",
            "100%|##########| 13/13 [00:13<00:00,  1.03s/it]\n"
          ]
        }
      ],
      "source": [
        "# 0. 足りない部品をインストール（これを追加しました！）\n",
        "!pip install faster-whisper\n",
        "\n",
        "# 元となる音声ファイル（wav形式）を入れるディレクトリ\n",
        "input_dir = \"/content/Style-Bert-VITS2/inputs\"\n",
        "# モデル名（話者名）を入力\n",
        "model_name = \"AIchan-2025Autumn-v1\"\n",
        "\n",
        "# こういうふうに書き起こして欲しいという例文（句読点の入れ方・笑い方や固有名詞等）\n",
        "initial_prompt = \"こんにちは。元気ですか？　ふふっ、私は…ちゃんと元気だよ！！\"\n",
        "\n",
        "!python slice.py -i \"{input_dir}\" --model_name \"{model_name}\"\n",
        "!python transcribe.py --model_name \"{model_name}\" --initial_prompt \"{initial_prompt}\" --language \"ja\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7vEWewoAWvS"
      },
      "source": [
        "成功したらそのまま3へ進んでください"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3AC-3zpAWvS"
      },
      "source": [
        "### 2.2 音声ファイルと書き起こしデータがすでにある場合\n",
        "\n",
        "指示に従って適切にデータセットを配置してください。\n",
        "\n",
        "次のセルを実行して、学習データをいれるフォルダ（1で設定した`dataset_root`）を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCNJl704h52"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(dataset_root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDgJCjCAWvT"
      },
      "source": [
        "まず音声データと、書き起こしテキストを用意してください。\n",
        "\n",
        "それを次のように配置します。\n",
        "```\n",
        "├── Data/\n",
        "│   ├── {モデルの名前}\n",
        "│   │   ├── esd.list\n",
        "│   │   ├── raw/\n",
        "│   │   │   ├── foo.wav\n",
        "│   │   │   ├── bar.mp3\n",
        "│   │   │   ├── style1/\n",
        "│   │   │   │   ├── baz.wav\n",
        "│   │   │   │   ├── qux.wav\n",
        "│   │   │   ├── style2/\n",
        "│   │   │   │   ├── corge.wav\n",
        "│   │   │   │   ├── grault.wav\n",
        "...\n",
        "```\n",
        "\n",
        "### 配置の仕方\n",
        "- 上のように配置すると、`style1/`と`style2/`フォルダの内部（直下以外も含む）に入っている音声ファイルたちから、自動的にデフォルトスタイルに加えて`style1`と`style2`というスタイルが作成されます\n",
        "- 特にスタイルを作る必要がない場合や、スタイル分類機能等でスタイルを作る場合は、`raw/`フォルダ直下に全てを配置してください。このように`raw/`のサブディレクトリの個数が0または1の場合は、スタイルはデフォルトスタイルのみが作成されます。\n",
        "- 音声ファイルのフォーマットはwav形式以外にもmp3等の多くの音声ファイルに対応しています\n",
        "\n",
        "### 書き起こしファイル`esd.list`\n",
        "\n",
        "`Data/{モデルの名前}/esd.list` ファイルには、以下のフォーマットで各音声ファイルの情報を記述してください。\n",
        "\n",
        "\n",
        "```\n",
        "path/to/audio.wav(wavファイル以外でもこう書く)|{話者名}|{言語ID、ZHかJPかEN}|{書き起こしテキスト}\n",
        "```\n",
        "\n",
        "- ここで、最初の`path/to/audio.wav`は、`raw/`からの相対パスです。つまり、`raw/foo.wav`の場合は`foo.wav`、`raw/style1/bar.wav`の場合は`style1/bar.wav`となります。\n",
        "- 拡張子がwavでない場合でも、`esd.list`には`wav`と書いてください、つまり、`raw/bar.mp3`の場合でも`bar.wav`と書いてください。\n",
        "\n",
        "\n",
        "例：\n",
        "```\n",
        "foo.wav|hanako|JP|こんにちは、元気ですか？\n",
        "bar.wav|taro|JP|はい、聞こえています……。何か用ですか？\n",
        "style1/baz.wav|hanako|JP|今日はいい天気ですね。\n",
        "style1/qux.wav|taro|JP|はい、そうですね。\n",
        "...\n",
        "english_teacher.wav|Mary|EN|How are you? I'm fine, thank you, and you?\n",
        "...\n",
        "```\n",
        "もちろん日本語話者の単一話者データセットでも構いません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r85-W20ECcr"
      },
      "source": [
        "## 3. 学習の前処理\n",
        "\n",
        "次に学習の前処理を行います。必要なパラメータをここで指定します。次のセルに設定等を入力して実行してください。「～～かどうか」は`True`もしくは`False`を指定してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CXR7kjuF5GlE"
      },
      "outputs": [],
      "source": [
        "# 上でつけたフォルダの名前`Data/{model_name}/`\n",
        "model_name = \"AIchan-2025Autumn-v1\"\n",
        "\n",
        "# JP-Extra （日本語特化版）を使うかどうか。日本語の能力が向上する代わりに英語と中国語は使えなくなります。\n",
        "use_jp_extra = True\n",
        "\n",
        "# 学習のバッチサイズ。VRAMのはみ出具合に応じて調整してください。\n",
        "batch_size = 4\n",
        "\n",
        "# 学習のエポック数（データセットを合計何周するか）。\n",
        "# 100で多すぎるほどかもしれませんが、もっと多くやると質が上がるのかもしれません。\n",
        "epochs = 100\n",
        "\n",
        "# 保存頻度。何ステップごとにモデルを保存するか。分からなければデフォルトのままで。\n",
        "save_every_steps = 1000\n",
        "\n",
        "# 音声ファイルの音量を正規化するかどうか\n",
        "normalize = True\n",
        "\n",
        "# 音声ファイルの開始・終了にある無音区間を削除するかどうか\n",
        "trim = True\n",
        "\n",
        "# 読みのエラーが出た場合にどうするか。\n",
        "# \"raise\"ならテキスト前処理が終わったら中断、\"skip\"なら読めない行は学習に使わない、\"use\"なら無理やり使う\n",
        "yomi_error = \"skip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFZdLTtpAWvT"
      },
      "source": [
        "上のセルが実行されたら、次のセルを実行して学習の前処理を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMVaOIPLabV5",
        "outputId": "61991e57-38b3-4f24-d7a7-9e8680352740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-20 14:29:13 |  INFO  | train.py:72 | Step 1: start initialization...\n",
            "model_name: AIchan-2025Autumn-v1, batch_size: 4, epochs: 100, save_every_steps: 1000, freeze_ZH_bert: False, freeze_JP_bert: False, freeze_EN_bert: False, freeze_style: False, freeze_decoder: False, use_jp_extra: True\n",
            "11-20 14:29:13 |WARNING | train.py:103 | Step 1: /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models already exists, so copy it to backup to /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models_backup\n",
            "11-20 14:29:23 |SUCCESS | train.py:132 | Step 1: initialization finished.\n",
            "11-20 14:29:23 |  INFO  | train.py:137 | Step 2: start resampling...\n",
            "11-20 14:29:23 |  INFO  | subprocess.py:23 | Running: resample.py -i /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/raw -o /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/wavs --num_processes 2 --sr 44100 --normalize --trim\n",
            "11-20 14:29:41 |SUCCESS | subprocess.py:38 | Success: resample.py -i /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/raw -o /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/wavs --num_processes 2 --sr 44100 --normalize --trim\n",
            "11-20 14:29:41 |SUCCESS | train.py:163 | Step 2: resampling finished.\n",
            "11-20 14:29:41 |  INFO  | train.py:170 | Step 3: start preprocessing text...\n",
            "11-20 14:29:41 |  INFO  | subprocess.py:23 | Running: preprocess_text.py --config-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/config.json --transcription-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/esd.list --train-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/train.list --val-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/val.list --val-per-lang 0 --yomi_error skip --correct_path --use_jp_extra\n",
            "11-20 14:29:58 |WARNING | subprocess.py:36 | Warning: preprocess_text.py --config-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/config.json --transcription-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/esd.list --train-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/train.list --val-path /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/val.list --val-per-lang 0 --yomi_error skip --correct_path --use_jp_extra\n",
            "2025-11-20 14:29:49.971999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763648989.992866   20930 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763648989.999319   20930 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763648990.016886   20930 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763648990.016909   20930 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763648990.016912   20930 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763648990.016914   20930 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-20 14:29:50.021802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "11-20 14:29:58 |WARNING | train.py:205 | Step 3: preprocessing text finished with stderr.\n",
            "11-20 14:29:58 |  INFO  | train.py:215 | Step 4: start bert_gen...\n",
            "11-20 14:29:58 |  INFO  | subprocess.py:23 | Running: bert_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/config.json\n",
            "11-20 14:36:04 |WARNING | subprocess.py:36 | Warning: bert_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/config.json\n",
            "2025-11-20 14:30:05.827037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763649005.860985   21024 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763649005.871407   21024 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763649005.896208   21024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763649005.896244   21024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763649005.896252   21024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763649005.896259   21024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-20 14:30:05.903460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "11-20 14:36:04 |WARNING | train.py:224 | Step 4: bert_gen finished with stderr.\n",
            "11-20 14:36:04 |  INFO  | train.py:234 | Step 5: start style_gen...\n",
            "11-20 14:36:04 |  INFO  | subprocess.py:23 | Running: style_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/config.json --num_processes 2\n",
            "11-20 14:36:20 |WARNING | subprocess.py:36 | Warning: style_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/config.json --num_processes 2\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:182: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep=\"\\s+\",\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:216: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep=\"\\s+\",\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:253: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep=\"\\s+\",\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:284: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  data = pd.read_csv(file_uem, names=names, dtype=dtype, sep=\"\\s+\")\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:309: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  data = pd.read_csv(path, names=names, dtype=dtype, sep=\"\\s+\")\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/loader.py:93: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  file_trial, sep=\"\\s+\", names=[\"reference\", \"uri1\", \"uri2\"]\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/loader.py:292: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  ctm, names=names, dtype=dtype, sep=\"\\s+\"\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/loader.py:357: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  mapping, names=names, dtype=dtype, sep=\"\\s+\"\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "11-20 14:36:20 |WARNING | train.py:252 | Step 5: style_gen finished with stderr.\n",
            "11-20 14:36:20 |SUCCESS | train.py:321 | Success: All preprocess finished!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Success: 全ての前処理が完了しました。ターミナルを確認しておかしいところがないか確認するのをおすすめします。')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from gradio_tabs.train import preprocess_all\n",
        "from style_bert_vits2.nlp.japanese import pyopenjtalk_worker\n",
        "\n",
        "\n",
        "pyopenjtalk_worker.initialize_worker()\n",
        "\n",
        "preprocess_all(\n",
        "    model_name=model_name,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    save_every_steps=save_every_steps,\n",
        "    num_processes=2,\n",
        "    normalize=normalize,\n",
        "    trim=trim,\n",
        "    freeze_EN_bert=False,\n",
        "    freeze_JP_bert=False,\n",
        "    freeze_ZH_bert=False,\n",
        "    freeze_style=False,\n",
        "    freeze_decoder=False,\n",
        "    use_jp_extra=use_jp_extra,\n",
        "    val_per_lang=0,\n",
        "    log_interval=200,\n",
        "    yomi_error=yomi_error,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVhwI5C-AWvT"
      },
      "source": [
        "## 4. 学習\n",
        "\n",
        "前処理が正常に終わったら、学習を行います。次のセルを実行すると学習が始まります。\n",
        "\n",
        "学習の結果は、上で指定した`save_every_steps`の間隔で、Google Driveの中の`Style-Bert-VITS2/Data/{モデルの名前}/model_assets/`フォルダに保存されます。\n",
        "\n",
        "このフォルダをダウンロードし、ローカルのStyle-Bert-VITS2の`model_assets`フォルダに上書きすれば、学習結果を使うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "laieKrbEb6Ij"
      },
      "outputs": [],
      "source": [
        "# 上でつけたモデル名を入力。学習を途中からする場合はきちんとモデルが保存されているフォルダ名を入力。\n",
        "model_name = \"AIchan-2025Autumn-v1\"\n",
        "\n",
        "\n",
        "import yaml\n",
        "from gradio_tabs.train import get_path\n",
        "\n",
        "paths = get_path(model_name)\n",
        "dataset_path = str(paths.dataset_path)\n",
        "config_path = str(paths.config_path)\n",
        "\n",
        "with open(\"default_config.yml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    yml_data = yaml.safe_load(f)\n",
        "yml_data[\"model_name\"] = model_name\n",
        "with open(\"config.yml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump(yml_data, f, allow_unicode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqGeHNabAWvT",
        "outputId": "9ef53cb7-e048-430f-89bb-db60c2a1d653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-20 14:42:38.616174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763649758.648366   24175 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763649758.658854   24175 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763649758.682437   24175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763649758.682470   24175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763649758.682476   24175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763649758.682483   24175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-20 14:42:38.689717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 0\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config localhost\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 10086\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 0\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 1\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:120 | Loading environment variables \n",
            "MASTER_ADDR: localhost,\n",
            "MASTER_PORT: 10086,\n",
            "WORLD_SIZE: 1,\n",
            "RANK: 0,\n",
            "LOCAL_RANK: 0\n",
            "11-20 14:42:48 |  INFO  | default_style.py:54 | At least 2 subdirectories are required for generating style vectors with respect to them, found 0.\n",
            "11-20 14:42:48 |  INFO  | default_style.py:57 | Generating only neutral style vector instead.\n",
            "11-20 14:42:48 |  INFO  | default_style.py:28 | Saved mean style vector to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1\n",
            "11-20 14:42:48 |  INFO  | default_style.py:36 | Saved style config to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1/config.json\n",
            "11-20 14:42:48 |WARNING | __init__.py:247 | /content/Style-Bert-VITS2/style_bert_vits2/models/utils is not a git repository, therefore hash value comparison will be ignored.\n",
            "11-20 14:42:48 |  INFO  | data_utils.py:69 | Init dataset...\n",
            "100% 13/13 [00:00<00:00, 3844.19it/s]\n",
            "11-20 14:42:48 |  INFO  | data_utils.py:84 | skipped: 0, total: 13\n",
            "11-20 14:42:48 |  INFO  | data_utils.py:348 | Bucket info: [4, 4, 4, 4, 4, 4]\n",
            "11-20 14:42:48 |  INFO  | data_utils.py:69 | Init dataset...\n",
            "0it [00:00, ?it/s]\n",
            "11-20 14:42:48 |  INFO  | data_utils.py:84 | skipped: 0, total: 0\n",
            "11-20 14:42:48 |  INFO  | train_ms_jp_extra.py:287 | Using noise scaled MAS for VITS2\n",
            "11-20 14:42:52 |WARNING | safetensors.py:43 | Missing key: enc_p.style_proj.weight\n",
            "11-20 14:42:52 |WARNING | safetensors.py:43 | Missing key: enc_p.style_proj.bias\n",
            "11-20 14:42:52 |WARNING | safetensors.py:43 | Missing key: emb_g.weight\n",
            "11-20 14:42:52 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models/G_0.safetensors'\n",
            "11-20 14:42:53 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models/D_0.safetensors'\n",
            "11-20 14:42:53 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models/WD_0.safetensors'\n",
            "11-20 14:42:53 |  INFO  | train_ms_jp_extra.py:505 | Loaded the pretrained models.\n",
            "11-20 14:42:56 |  INFO  | train_ms_jp_extra.py:553 | Start training.\n",
            "Epoch 100(83%)/100: 100%|##########| 600/600 [16:34<00:00,  1.63s/it]11-20 14:59:31 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models/G_600.pth\n",
            "11-20 14:59:35 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models/D_600.pth\n",
            "11-20 14:59:37 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/AIchan-2025Autumn-v1/models/WD_600.pth\n",
            "11-20 14:59:38 |  INFO  | safetensors.py:90 | Saved safetensors to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1/AIchan-2025Autumn-v1_e100_s600.safetensors\n",
            "Epoch 100(83%)/100: 100%|##########| 600/600 [16:42<00:00,  1.67s/it]\n",
            "[rank0]:[W1120 14:59:40.697497375 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "# 日本語特化版を「使う」場合\n",
        "!python train_ms_jp_extra.py --config {config_path} --model {dataset_path} --assets_root {assets_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVbjh-WPAWvU"
      },
      "outputs": [],
      "source": [
        "# 日本語特化版を「使わない」場合\n",
        "!python train_ms.py --config {config_path} --model {dataset_path} --assets_root {assets_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7g0hrdeP1Tl",
        "outputId": "eebafb4a-fba2-4c57-dcb8-24d72c085a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-20 15:00:12.758311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763650812.797587   28522 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763650812.811342   28522 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763650812.850573   28522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763650812.850611   28522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763650812.850620   28522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763650812.850626   28522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-20 15:00:12.859823: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "11-20 15:00:21 | DEBUG  | __init__.py:130 | pyopenjtalk worker server started\n",
            "theme_schema%401.2.2.json: 12.0kB [00:00, 47.6MB/s]\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://d2ea35bd828009ef49.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "11-20 15:02:47 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:02:47 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは、初めまして。あなたの名前はなんていうの？\n",
            "11-20 15:02:47 |  INFO  | infer.py:26 | Using JP-Extra model\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "11-20 15:02:49 |  INFO  | safetensors.py:51 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1/AIchan-2025Autumn-v1_e100_s600.safetensors' (iteration 100)\n",
            "11-20 15:02:49 |  INFO  | tts_model.py:152 | Model loaded successfully from /content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1/AIchan-2025Autumn-v1_e100_s600.safetensors to \"cuda\" device (1.59s)\n",
            "11-20 15:02:49 |  INFO  | tts_model.py:196 | Null models merged successfully (1.59s)\n",
            "11-20 15:02:49 |  INFO  | bert_models.py:178 | Loaded the JP BERT tokenizer from /content/Style-Bert-VITS2/bert/deberta-v2-large-japanese-char-wwm\n",
            "11-20 15:08:40 |  INFO  | bert_models.py:112 | Loaded the JP BERT model from /content/Style-Bert-VITS2/bert/deberta-v2-large-japanese-char-wwm (351.19s)\n",
            "11-20 15:08:41 |  INFO  | tts_model.py:557 | Audio data generated successfully (354.18s)\n",
            "11-20 15:08:41 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:08:41 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！\n",
            "11-20 15:08:41 |  INFO  | infer.py:26 | Using JP-Extra model\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "11-20 15:08:42 |  INFO  | safetensors.py:51 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1/AIchan-2025Autumn-v1_e100_s600.safetensors' (iteration 100)\n",
            "11-20 15:08:42 |  INFO  | tts_model.py:152 | Model loaded successfully from /content/drive/MyDrive/Style-Bert-VITS2/model_assets/AIchan-2025Autumn-v1/AIchan-2025Autumn-v1_e100_s600.safetensors to \"cuda\" device (1.28s)\n",
            "11-20 15:08:42 |  INFO  | tts_model.py:196 | Null models merged successfully (1.28s)\n",
            "11-20 15:08:43 |  INFO  | tts_model.py:557 | Audio data generated successfully (1.61s)\n",
            "11-20 15:12:24 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:12:24 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんです！\n",
            "11-20 15:12:24 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.34s)\n",
            "11-20 15:12:37 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:12:37 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんです！\n",
            "11-20 15:12:38 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.50s)\n",
            "11-20 15:12:57 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:12:57 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんです！\n",
            "11-20 15:12:58 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.19s)\n",
            "11-20 15:13:13 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:13:13 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんです！\n",
            "11-20 15:13:13 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.21s)\n",
            "11-20 15:13:26 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:13:26 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんでs！\n",
            "11-20 15:13:27 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.23s)\n",
            "11-20 15:13:52 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:13:52 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんだよ！\n",
            "11-20 15:13:52 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.24s)\n",
            "11-20 15:14:11 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:14:11 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんだよ！\n",
            "11-20 15:14:11 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.22s)\n",
            "11-20 15:14:25 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:14:25 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんだよ！\n",
            "11-20 15:14:25 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.20s)\n",
            "11-20 15:14:37 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:14:37 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんだよ！\n",
            "11-20 15:14:38 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.21s)\n",
            "11-20 15:14:47 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "11-20 15:14:47 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは！　あいちゃんだよ！\n",
            "11-20 15:14:47 |  INFO  | tts_model.py:557 | Audio data generated successfully (0.21s)\n"
          ]
        }
      ],
      "source": [
        "# 学習結果を試す・マージ・スタイル分けはこちらから\n",
        "!python app.py --share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgaRsIa-fdPJ"
      },
      "outputs": [],
      "source": [
        "# ONNX変換は、変換したいsafetensorsファイルを指定してこのセルを実行してください。\n",
        "!python convert_onnx.py --model \"Data/your_model/your_model_e100_s10000.safetensors\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}